# Simpsons Character Classification Project Report

## I. Experimental Objective | ä¸€ã€å¯¦é©—ç›®çš„
This project aims to build a deep learning model capable of automatically identifying **50 characters** from *The Simpsons*.  
By leveraging the **ResNet101** pre-trained model along with various data augmentation and regularization techniques, the model's classification accuracy under diverse facial expressions and background conditions is significantly improved.  
The final model achieves **91% top-1 accuracy** on the validation set, demonstrating the effectiveness of the applied strategies.  
æœ¬å°ˆæ¡ˆæ—¨åœ¨å»ºç«‹ä¸€å€‹èƒ½è‡ªå‹•è¾¨è­˜ã€Šè¾›æ™®æ£®å®¶åº­ã€‹(The Simpsons) ä¸­ **50 ä½è§’è‰²** çš„æ·±åº¦å­¸ç¿’æ¨¡å‹ã€‚  
é€é **ResNet101** é è¨“ç·´æ¨¡å‹ï¼Œçµåˆå¤šæ¨£åŒ–è³‡æ–™å¢å¼·èˆ‡æ­£å‰‡åŒ–æŠ€è¡“ï¼Œæå‡æ¨¡å‹åœ¨å¤šæ¨£è‡‰éƒ¨è¡¨æƒ…èˆ‡èƒŒæ™¯æ¢ä»¶ä¸‹çš„è¾¨è­˜æº–ç¢ºç‡ã€‚  
æœ€çµ‚æ¨¡å‹æ–¼é©—è­‰é›†ä¸Šé”åˆ° **91% æº–ç¢ºç‡ (Top-1 Accuracy)**ï¼Œé¡¯ç¤ºæ‰€æ¡ç­–ç•¥èƒ½é¡¯è‘—æ”¹å–„åˆ†é¡è¡¨ç¾ã€‚

---

## II. Dataset and Preprocessing | äºŒã€è³‡æ–™é›†èˆ‡å‰è™•ç†

### (1) Dataset Source | (1) è³‡æ–™ä¾†æº
- Dataset: *The Simpsons* character images (approx. **97,000 images** in total)  
- Number of Classes: **50 characters**  
- Training/Validation Split: **70%/30%**  
- The class distribution is approximately balanced.  
è³‡æ–™é›†ç‚ºã€ŠThe Simpsonsã€‹50 åä¸»è¦è§’è‰²å½±åƒï¼Œç¸½è¨ˆç´„ **97,000 å¼µåœ–ç‰‡**ã€‚  
æ¯å€‹è§’è‰²å¹³å‡ç´„ 1,800â€“2,000 å¼µå½±åƒï¼Œè³‡æ–™åˆ†å¸ƒå‡è¡¡ã€‚

### (2) Data Augmentation Strategy | (2) è³‡æ–™å¢å¼·ç­–ç•¥
| Type | Technique | Description |  
|------|-----------|-------------|  
| Geometric Transformations | RandomResizedCrop, Rotation, Perspective, ElasticTransform | Simulate different angles and scales |  
| Color Augmentations | ColorJitter, Grayscale, Invert, Solarize | Simulate different lighting and color environments |  
| Background Mixing | BackgroundMixing (p=0.35) | Mix character with random background to reduce background bias |  
| Noise Handling | GaussianBlur, RandomErasing | Simulate noise and occlusion |  
| Combination Techniques | MixUp, CutMix | Combine sample features and labels |  
| Proportions | Weak Augmentation 30%, Strong Augmentation 70% |  
| é¡åˆ¥ | æ–¹æ³• | èªªæ˜ |  
|------|------|------|  
| å¹¾ä½•è®Šæ› | RandomResizedCropã€Rotationã€Perspectiveã€ElasticTransform | æ¨¡æ“¬å¤šè§’åº¦èˆ‡æ¯”ä¾‹å·®ç•° |  
| é¡è‰²èª¿æ•´ | ColorJitterã€Grayscaleã€Invertã€Solarize | æ¨¡æ“¬ä¸åŒå…‰ç·šèˆ‡é¡è‰²ç’°å¢ƒ |  
| èƒŒæ™¯æ··åˆ | BackgroundMixing (p=0.35) | å°‡è§’è‰²èˆ‡éš¨æ©ŸèƒŒæ™¯èåˆï¼Œé™ä½èƒŒæ™¯åå·® |  
| é›œè¨Šèˆ‡é®è”½ | GaussianBlurã€RandomErasing | æ¨¡æ“¬æ¨¡ç³Šèˆ‡é®æ“‹ |  
| è³‡æ–™æ··åˆ | MixUpã€CutMix | çµåˆæ¨£æœ¬ç‰¹å¾µèˆ‡æ¨™ç±¤ |  
| ä½¿ç”¨æ¯”ä¾‹ | åŸåœ–å¼±å¢å¼· 30% ï¼šå¼·å¢å¼· 70% |

This strategy significantly improves the model's robustness to **varied backgrounds and facial expressions**.  
æ­¤ç­–ç•¥å¯é¡¯è‘—æå‡æ¨¡å‹å° **å¤šæ¨£å ´æ™¯èˆ‡è¡¨æƒ…è®ŠåŒ–** çš„é­¯æ£’æ€§ã€‚

---

## III. Model Architecture and Training Configuration | ä¸‰ã€æ¨¡å‹æ¶æ§‹èˆ‡è¨“ç·´è¨­å®š

| Item | Setting |  
|------|---------|  
| Model Architecture | ResNet101 (Pre-trained on ImageNet) |  
| Optimizer | AdamW |  
| Loss Function | CrossEntropyLoss (Label Smoothing = 0.03) |  
| Batch Size | 16 |  
| Initial Learning Rate | 0.0002 |  
| Weight Decay | 0.0003 |  
| Epochs | 25 |  
| Learning Rate Strategy | ReduceLROnPlateau |  
| Learning Rate Adjustment | FC layers Ã—1.0, other layers Ã—0.1 |  
| Freezing Strategy | Initially freeze all layers, only train `fc`; Unfreeze `layer4` at epoch 5 |  
| Stability Mechanism | EMA (Exponential Moving Average) |  

---

## IV. Training Process | å››ã€è¨“ç·´æµç¨‹
1. Initialize the ResNet101 model and load ImageNet weights.  
2. Freeze all convolutional layers and train only the fully connected (`fc`) layers.  
3. Unfreeze `layer4` at epoch 5 to fine-tune higher-level semantic features.  
4. Apply **differentiated learning rates**:  
 - Higher learning rate for `fc` layers to adapt to the new task.  
 - Lower learning rate for other layers to retain ImageNet features.  
5. Use **MixUp** and **CutMix** to enhance sample diversity.  
6. Monitor accuracy and loss after each epoch using the validation set and save the best weights.  

1. åˆå§‹åŒ– ResNet101 æ¨¡å‹ä¸¦è¼‰å…¥ ImageNet æ¬Šé‡ã€‚  
2. å‡çµæ‰€æœ‰å·ç©å±¤ï¼Œåªè¨“ç·´æœ€å¾Œçš„å…¨é€£æ¥å±¤ (`fc`)ã€‚  
3. ç¬¬ 5 å€‹ epoch è‡ªå‹•è§£å‡ `layer4`ï¼Œè®“æ¨¡å‹èƒ½é€²ä¸€æ­¥å¾®èª¿é«˜éšèªç¾©ç‰¹å¾µã€‚  
4. é€é **å·®ç•°åŒ–å­¸ç¿’ç‡**ï¼š  
 - `fc` å±¤å­¸ç¿’ç‡è¼ƒé«˜ï¼Œä»¥é©æ‡‰æ–°ä»»å‹™ã€‚  
 - å…¶ä»–å±¤å­¸ç¿’ç‡è¼ƒä½ï¼Œä¿ç•™é€šç”¨ç‰¹å¾µã€‚  
5. é€é **MixUp** èˆ‡ **CutMix** æé«˜æ¨£æœ¬å¤šæ¨£æ€§ã€‚  
6. æ¯è¼ªè¨“ç·´å¾Œä»¥é©—è­‰é›†ç›£æ§æº–ç¢ºç‡èˆ‡ Lossï¼Œä¸¦è‡ªå‹•å„²å­˜æœ€ä½³æ¬Šé‡ã€‚

---

## V. Experimental Results | äº”ã€å¯¦é©—çµæœ

### (1) Validation Performance | (1) é©—è­‰é›†è¡¨ç¾
| Metric | Value |  
|--------|-------|  
| Top-1 Accuracy | **0.91** |  
| Loss | 0.25 |  
| Confusion Matrix Size | 50 Ã— 50 |  

The model performs consistently across most characters, with minor misclassifications involving visually similar characters:  
æ¨¡å‹åœ¨å¤§éƒ¨åˆ†è§’è‰²çš„åˆ†é¡ä¸Šè¡¨ç¾ç©©å®šï¼Œå°‘æ•¸èª¤åˆ¤é›†ä¸­æ–¼å¤–è§€ç›¸ä¼¼è§’è‰²ï¼š
- `bart_simpson` â†” `lisa_simpson`  
- `homer_simpson` â†” `abraham_grampa_simpson`  
- `nelson_muntz` â†” `barney_gumble`

---

### (2) Confusion Matrix | (2) æ··æ·†çŸ©é™£
The confusion matrix for the modelâ€™s predictions on the validation set:  
æ¨¡å‹é æ¸¬çµæœçš„æ··æ·†çŸ©é™£å¦‚ä¸‹åœ–ï¼Œæ©«è»¸ç‚ºçœŸå¯¦é¡åˆ¥ã€ç¸±è»¸ç‚ºé æ¸¬é¡åˆ¥ï¼š  

![Confusion Matrix](confusion_matrix.png)

---

### (3) Visualization and Understanding Convolutional Neural Networks | (3) è¦–è¦ºåŒ–èˆ‡å·ç©ç¥ç¶“ç¶²çµ¡
- Successfully visualized the **3x3** and **1x1 convolution kernels** from the final convolutional layer, which helps understand how the model extracts features.  
- Below are the feature maps, which show how the model activates for different regions of the face.

- æˆåŠŸè¦–è¦ºåŒ–äº†æ¨¡å‹çš„æœ€å¾Œä¸€å±¤å·ç©æ ¸ï¼ˆ3x3ï¼‰å’Œ1x1å·ç©æ ¸ï¼Œé€™æœ‰åŠ©æ–¼ç†è§£æ¨¡å‹å¦‚ä½•æå–ç‰¹å¾µã€‚  
- ä¸‹æ–¹é‚„å±•ç¤ºäº†ç‰¹å¾µåœ–ï¼Œé€™äº›ç‰¹å¾µåœ–é¡¯ç¤ºäº†æ¨¡å‹å°ä¸åŒé¢éƒ¨å€åŸŸçš„åæ‡‰ï¼Œä¸¦å±•ç¤ºäº†æ¨¡å‹åœ¨ä¸åŒå±¤æ¬¡å¦‚ä½•é€æ­¥å­¸ç¿’åˆ°æ›´é«˜ç´šçš„ç‰¹å¾µã€‚

#### (a) 3x3 Filter Weights (Convolution Kernels) | (a) 3x3 æ¿¾æ³¢å™¨çš„æ¬Šé‡ï¼ˆå·ç©æ ¸ï¼‰
![3x3 Conv Kernels](layer4_conv2_kernels_3x3.png)  
This image shows the **3x3 filters** from the final convolutional layer (conv2) of the ResNet model. These filters represent how the model learns to detect local features in the input image, such as edges and color changes.  
é€™å¼µåœ–ç‰‡é¡¯ç¤ºäº† ResNet æ¨¡å‹ä¸­æœ€å¾Œä¸€å±¤å·ç©å±¤ (conv2) çš„ 3x3 æ¿¾æ³¢å™¨ã€‚é€™äº›æ¿¾æ³¢å™¨ä»£è¡¨äº†æ¨¡å‹å¦‚ä½•å­¸ç¿’åˆ°è¼¸å…¥åœ–åƒä¸­çš„å±€éƒ¨ç‰¹å¾µï¼Œä¾‹å¦‚é‚Šç·£ã€é¡è‰²è®ŠåŒ–ç­‰ã€‚

#### (b) 1x1 Filter Weights (Convolution Kernels) | (b) 1x1 æ¿¾æ³¢å™¨çš„æ¬Šé‡ï¼ˆå·ç©æ ¸ï¼‰
![1x1 Conv Kernels](layer4_conv3_kernels_1x1.png)  
This image shows the **1x1 filters** from the final convolutional layer. These filters are typically used for channel-wise semantic mixing, and although their spatial structure is less intuitive than 3x3 filters, they play a crucial role in feature extraction and channel aggregation.  
é€™å¼µåœ–ç‰‡é¡¯ç¤ºäº† ResNet æ¨¡å‹ä¸­æœ€å¾Œä¸€å±¤ 1x1 å·ç©å±¤çš„æ¿¾æ³¢å™¨ã€‚é€™äº›æ¿¾æ³¢å™¨é€šå¸¸ç”¨æ–¼é€²è¡Œé€šé“é–“çš„èªç¾©æ··åˆï¼Œé›–ç„¶å®ƒå€‘çš„ç©ºé–“çµæ§‹ä¸å¦‚ 3x3 æ¿¾æ³¢å™¨ç›´è§€ï¼Œä½†ä»ç„¶å°ç‰¹å¾µæå–å’Œé€šé“èšåˆæœ‰è‘—é‡è¦ä½œç”¨ã€‚

#### (c) Feature Map from the Last Layer | (c) æœ€å¾Œä¸€å±¤çš„ç‰¹å¾µåœ–ï¼ˆFeature Mapï¼‰
![Feature Map 1](layer4_feature_map_7x7.png)  
This image shows the **feature map** after the final convolutional layer. Each feature map reflects the modelâ€™s activation for certain regions of the image, which may correspond to key features (such as facial parts).  
é€™å¼µåœ–é¡¯ç¤ºäº†æ¨¡å‹åœ¨é€²è¡Œå‰å‘å‚³æ’­æ™‚ï¼Œç¶“éæœ€å¾Œä¸€å±¤å·ç©å±¤çš„ç‰¹å¾µåœ–ã€‚æ¯å€‹ç‰¹å¾µåœ–åæ˜ äº†æ¨¡å‹å°åœ–åƒä¸­æŸäº›å€åŸŸçš„æ¿€æ´»ï¼Œé€™äº›å€åŸŸå¯èƒ½èˆ‡ç‰¹å¾µï¼ˆå¦‚è‡‰éƒ¨éƒ¨ä½ï¼‰ç›¸é—œã€‚

#### (d) Another Feature Map from the Same Layer | (d) å¦ä¸€å€‹ç‰¹å¾µåœ–ï¼ˆFeature Mapï¼‰
![Feature Map 2](layer4_feature_map_7x7_v2.png)  
This image shows another feature map extracted from the same convolutional layer. Similar to the previous one, it visualizes how the model responds to different areas of the image.  
é€™å¼µåœ–å±•ç¤ºäº†å¾åŒä¸€å±¤å·ç©ä¸­æå–çš„å¦ä¸€å€‹ç‰¹å¾µåœ–ï¼Œèˆ‡ä¸Šä¸€å¼µç‰¹å¾µåœ–ç›¸ä¼¼ï¼Œä¹Ÿç”¨ä¾†å±•ç¤ºæ¨¡å‹å°ä¸åŒå€åŸŸçš„æ¿€æ´»æƒ…æ³ã€‚

---

## VI. Performance Comparison and Analysis | å…­ã€æ•ˆèƒ½æ¯”è¼ƒèˆ‡æå‡åˆ†æ

| Model Version | Accuracy | Key Improvements |  
|---------------|----------|------------------|  
| baseline (final.ipynb) | 0.76 | No BackgroundMixing, MixUp |  
| improved (0.91_test.ipynb) | **0.91** | Added BackgroundMixing, MixUp/CutMix, Label Smoothing, EMA, Differentiated LR |  

**Key Reasons for Improvement:**
1. **Background Mixing** reduces background interference, allowing the model to focus more on the character features.  
2. **Differentiated Learning Rates** allow the higher layers to quickly learn the new task, while preserving the general features from ImageNet in the lower layers.  
3. **Label Smoothing** reduces model overconfidence on single classes, improving generalization.  
4. **Strong Augmentation (70%)** increases sample diversity, enhancing the model's robustness.  
5. **EMA** improves the stability and performance of the model during training.

---

## VII. Conclusion and Observations | ä¸ƒã€çµè«–èˆ‡è§€å¯Ÿ
- The model performs well in recognizing **The Simpsons** characters, achieving high accuracy even under diverse backgrounds and lighting conditions.  
- The enhanced data augmentation strategies significantly improve the model's generalization capability.  
- The differentiated learning rates and label smoothing prevent overfitting and ensure efficient training.  
- The overall accuracy improved from **0.76** (baseline) to **0.91**, indicating the significant impact of the applied strategies.  

æ¨¡å‹èƒ½æº–ç¢ºè¾¨è­˜ã€Šè¾›æ™®æ£®å®¶åº­ã€‹ä¸»è¦è§’è‰²ï¼Œä¸¦åœ¨ä¸åŒèƒŒæ™¯ã€å…‰ç·šä¸‹ç¶­æŒç©©å®šè¡¨ç¾ã€‚  
å¢å¼·ç­–ç•¥é¡¯è‘—æå‡æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚  
å·®ç•°åŒ–å­¸ç¿’ç‡ä½¿è¨“ç·´æ›´é«˜æ•ˆï¼Œé¿å…ä½å±¤ç‰¹å¾µè¢«ç ´å£ã€‚  
æ•´é«”æº–ç¢ºç‡ç”± **0.76**ï¼ˆbaselineï¼‰æå‡è‡³ **0.91**ï¼Œé¡¯ç¤ºç­–ç•¥é¡¯è‘—æ”¹å–„ã€‚

---

## VIII. Future Directions | å…«ã€æœªä¾†å±•æœ›
1. Implement **Grad-CAM** or other explainability techniques to visualize the regions the model focuses on.  
2. Explore **semi-supervised learning** to handle unknown characters.  
3. Extend the modelâ€™s application to **multi-character detection** or **scene classification** in animated series.  
4. Further optimize the **Background Mixing** strategy to reduce semantic misalignment.

1. åŠ å…¥ **Grad-CAM** æˆ–å…¶ä»–å¯è§£é‡‹æ€§æŠ€è¡“ï¼Œä»¥å¯è¦–åŒ–æ¨¡å‹é—œæ³¨å€åŸŸã€‚  
2. æ¢ç´¢ **åŠç›£ç£å­¸ç¿’** ä»¥è™•ç†æœªçŸ¥è§’è‰²ã€‚  
3. å»¶ä¼¸æ‡‰ç”¨è‡³ **å¤šè§’è‰²åŒæ¡†å ´æ™¯æª¢æ¸¬** æˆ– **å‹•ç•«é¡é ­åˆ†é¡**ã€‚  
4. é€²ä¸€æ­¥å„ªåŒ– **èƒŒæ™¯æ··åˆ** ç­–ç•¥ä»¥æ¸›å°‘èªç¾©éŒ¯é…ã€‚

---

## IX. Final Performance | ä¹ã€æœ€çµ‚æ•ˆèƒ½

> ğŸ¯ **Validation Accuracy:** **91%**  
> ğŸš€ **Loss:** **0.25**  
> ğŸ“ˆ **Model:** ResNet101 + MixUp + CutMix + LabelSmoothing + BackgroundMixing  

> ğŸ¯ **é©—è­‰æº–ç¢ºç‡ï¼š** **91%**  
> ğŸš€ **Lossï¼š** **0.25**  
> ğŸ“ˆ **æ¨¡å‹ï¼š** ResNet101 + MixUp + CutMix + LabelSmoothing + BackgroundMixing  

---

## X. Development Environment and Versions | åã€é–‹ç™¼ç’°å¢ƒèˆ‡ç‰ˆæœ¬

| Package | Version |  
|---------|---------|  
| Python | 3.10 |  
| torch | 2.1.2 |  
| torchvision | 0.16.2 |  
| numpy | 1.26.4 |  
| pandas | 2.2.1 |  
| scikit-learn | 1.5.1 |  
| matplotlib | 3.8.4 |  
| Pillow | 10.2.0 |  
| tqdm | 4.66.2 |  
| opencv-python | 4.9.0.80 |  

| å¥—ä»¶ | ç‰ˆæœ¬ |  
|------|------|  
| Python | 3.10 |  
| torch | 2.1.2 |  
| torchvision | 0.16.2 |  
| numpy | 1.26.4 |  
| pandas | 2.2.1 |  
| scikit-learn | 1.5.1 |  
| matplotlib | 3.8.4 |  
| Pillow | 10.2.0 |  
| tqdm | 4.66.2 |  
| opencv-python | 4.9.0.80 |  

---

This concludes the report for the **Simpsons Character Classification** project.  
é€™æ˜¯ã€Šè¾›æ™®æ£®å®¶åº­è§’è‰²åˆ†é¡ã€‹å°ˆæ¡ˆçš„å ±å‘Šçµå°¾ã€‚

---
